{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import one_hot \n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.utils import pad_sequences\n",
    "from keras_tuner import HyperModel\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model\n",
    "from numpy.random import seed\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "MAX_WORD_LENGTH = 23\n",
    "SEED = 12"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layrer prep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA = \"Merged_data.csv\"\n",
    "\n",
    "df = pd.read_csv(ORIGINAL_DATA,delimiter=\",\")\n",
    "def preprocess_word_t(word_list):\n",
    "    return word_tokenize(re.sub(r'[^a-zA-Z]', ' ', word_list))  \n",
    "def uniq_val(clmn):\n",
    "    return np.unique([wrd for bag_wrd in clmn for wrd in bag_wrd ] )\n",
    "\n",
    "tf.keras.utils.set_random_seed(SEED)\n",
    "seed(SEED)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_stripper(txt):\n",
    "    return re.sub(r'[^a-zA-Z]', ' ', txt)\n",
    "\n",
    "\n",
    "def pre_processing (clmn): \n",
    "    # Tokenizing and stop word removal from column\n",
    "    procc_str = clmn.apply(lambda txt: [item for item in word_tokenize(num_stripper(txt)) if item.lower() not in stopwords.words('english')])\n",
    "    # returning the embedding size of column\n",
    "    embd_size = np.unique([wrd for bag_wrd in procc_str for wrd in bag_wrd ] )\n",
    "    # Creating inverse dictionary for each unique word\n",
    "    inv_dict  = dict(zip( embd_size,range(1,embd_size.size+1) ))\n",
    "    # Return Processed strings and inverse dictionary \n",
    "    return [procc_str, inv_dict]\n",
    "\n",
    "def onehot_padding(data,invs_dict):\n",
    "    new_df , res  = [],[]\n",
    "    # Loop over columns in dataframe\n",
    "    for clmn in data:\n",
    "        if clmn != 'Url' :\n",
    "            # One hot encoding each row in column\n",
    "            res= [[invs_dict[clmn][word] for word in strn] for strn in data[clmn] ]\n",
    "        else:\n",
    "            # One hot encoding each row in URL separately due to char removal\n",
    "            res = [[invs_dict[clmn][key]] for key in data[clmn]]\n",
    "        new_df.append(res) \n",
    "    # Padding sequence to max word length\n",
    "    new_df = [pad_sequences(clmn,maxlen= MAX_WORD_LENGTH , padding='post')for clmn in new_df]\n",
    "    return new_df\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing stop words and counting embeding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_conversion (df):\n",
    "    w_dict = {}\n",
    "    for clmn in df.iloc[:,:-1]:\n",
    "        x = []\n",
    "        if clmn not in ['Url' ,'Location' ]:\n",
    "            df[clmn],w_dict[clmn] = pre_processing (df[clmn])\n",
    "        elif clmn == 'Location':\n",
    "            locl_list = [re.split(r',(?=.)', item)  for item in df[clmn]]\n",
    "            df[clmn] = locl_list\n",
    "            unique_items = list(set(cnty for county_lst in locl_list for cnty in county_lst))\n",
    "            w_dict[clmn] = dict(zip(unique_items, range(1,len(unique_items)+1)))\n",
    "        else:\n",
    "            url_uniq = df[clmn].unique()\n",
    "            w_dict[clmn] = dict(zip(url_uniq, range(1,len(url_uniq)+1)))\n",
    "    return df , w_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_splitting(df):\n",
    "    X = df.iloc[: ,:-1]\n",
    "    # Target\n",
    "    y = df['CTR']\n",
    "    # # Train test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15 ,random_state=42)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size =  0.25 ,random_state=42)\n",
    "    # Check the number of records in training and testing dataset.\n",
    "    print(f'The training dataset shape {len(X_train)} and validation shape {len(X_val)} .')\n",
    "    print(f'The testing dataset has {len(X_test)} records.')\n",
    "    return X_train, X_test ,X_val, y_train, y_val ,y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training dataset shape 25 and validation shape 9 .\n",
      "The testing dataset has 6 records.\n",
      "The training dataset has (25, 13) records and (25, 13) columns.\n",
      "The testing dataset has 6 records.\n"
     ]
    }
   ],
   "source": [
    "df ,wrd_dict =  df_conversion(df)\n",
    "X = df.iloc[: ,:-1]\n",
    "# Target\n",
    "y = df['CTR']\n",
    "\n",
    "# # Train test split\n",
    "X_train, X_test,X_val,y_train, y_val ,y_train, y_test = data_splitting(df)\n",
    "# Check the number of records in training and testing dataset.\n",
    "print(f'The training dataset has {X_train.shape} records and {X_train.shape} columns.')\n",
    "print(f'The testing dataset has {len(X_test)} records.')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = onehot_padding(X_train,wrd_dict)\n",
    "X_val = onehot_padding(X_val,wrd_dict)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model making"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, max_vocab_size, max_sequence_length, num_inputs):\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "    def build(self, hp):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        # Define input layer\n",
    "        for sze,lst in zip(self.max_vocab_size,self.num_inputs) :\n",
    "            embed_dim = int(min(np.ceil(self.max_vocab_size[sze] /2), 45))\n",
    "            inp = layers.Input(shape= np.array(lst).shape[-1])\n",
    "            out = layers.Embedding(self.max_vocab_size[sze]+1, embed_dim)(inp)\n",
    "            out = layers.Dropout(hp.Float(\"intial drop\",min_value = 0.1, max_value = 0.9))(out)\n",
    "            inputs.append(inp)\n",
    "            outputs.append(out)\n",
    "\n",
    "        # Concatenate the embedding layers\n",
    "        x = layers.Concatenate()(outputs)\n",
    "\n",
    "        # Add Flatten layer\n",
    "        x = layers.Flatten()(x)\n",
    "\n",
    "        for i in range(hp.Int('num_of_layers',1,3)):         \n",
    "            # providing range for number of neurons in hidden layers\n",
    "            x = layers.Dense( units = hp.Int(\"units_\" + str(i), min_value=32, max_value=512, step=16), activation='relu')(x)\n",
    "            x = layers.Dropout(hp.Float(\"layer dropping\",min_value = 0.1, max_value = 0.9))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # Add Dense layer\n",
    "        y = layers.Dense(1, activation=\"linear\")(x)\n",
    "        model = Model(inputs=inputs, outputs=y)\n",
    "        # Compile the model\n",
    "        hp_learning_rate = hp.Float('learning_rate', min_value = 1e-4, max_value =1e-2, sampling='log')\n",
    "\n",
    "        model.compile(optimizer=keras.optimizers.Adam(learning_rate= 0.00142),\n",
    "                    loss='mean_squared_error',\n",
    "                    metrics=['mae'])\n",
    "\n",
    "        return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early stopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50)\n",
    "log_dir = \"logs/fit/dropout_rate\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_sizes = {x : len(wrd_dict[x]) for x in wrd_dict}\n",
    "mode_build = MyHyperModel(embedding_sizes,MAX_WORD_LENGTH,X_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tuner = kt.RandomSearch(\n",
    "    mode_build,\n",
    "    objective='val_loss',\n",
    "    max_trials=20,\n",
    "    overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_tuner.search(X_train, y_train, epochs=565 ,validation_data=(X_val, y_val), callbacks = [stop_early],batch_size=32)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Random search results{rand_tuner.results_summary()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
